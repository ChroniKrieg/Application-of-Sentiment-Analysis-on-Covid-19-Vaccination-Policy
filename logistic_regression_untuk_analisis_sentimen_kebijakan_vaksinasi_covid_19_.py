# -*- coding: utf-8 -*-
"""Logistic Regression untuk Analisis Sentimen Kebijakan Vaksinasi Covid-19 .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AogFHEvhKNz9-t1wP5Joiwws18kIdLtV

#**Load Library**
"""

from sklearn import preprocessing
import io
import numpy as np# linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.multiclass import OneVsRestClassifier
from sklearn.multiclass import OneVsOneClassifier

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix, classification_report

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')

"""#**Preprocessing**

## Data Preparation

### Load Data
"""

df = pd.read_csv("covid-19 tweet vaksin.csv", encoding= 'unicode_escape')
df

#from google.colab import drive
#drive.mount('/content/drive')

df.info()

"""### Visualization of dispersal of label data

> This step is used to known the number of data every label (1: Negative, 2:Netral, 3:Positive)


"""

#Calculate total every label 1;2;3


import seaborn as sns
import matplotlib.pyplot as plt

#GRAPHIC

# Plotting
plt.figure(figsize=(6, 5))

# Get the counts of each label
counts_series = df['label'].value_counts()
counts_dict = counts_series.to_dict()

# Plot the count plot
plot = sns.countplot(x='label', hue='label', data=df, palette='viridis', dodge=False)
# Add text annotations
for label, count in counts_dict.items():
    plt.text(label - 1, count + 0.1, str(count), ha='center', va='bottom', fontsize=8)

# Add title and labels to the plot
plt.title('Count of Data Sentiment')
plt.xlabel('Labels')
plt.ylabel('Count')

# Set custom tick labels for x-axis
plt.xticks(ticks=[0, 1,2],labels=['1', '2', '3'])

# Add legend
handles, labels = plot.get_legend_handles_labels()

labels = ['Negative', 'Netral', 'Positive']
plt.legend(handles, labels, title='Labels')
# Display the plot
plt.show()

"""from the graph it is found that the dataset is imbalanced, where the negative sentiment is only 420 while the positive sentiment is 1900. therefore I do the undersampling method by deleting in order the positive data becomes the same as the negative data becomes the same 420.

I intentionally make the number of positive and negative data have the same amount of data (drop method) so that the precision score does not have a significant difference between positive and negative data from imbalance data.

### Drop Unused data

```
# In this research is only used 2 label (Negative and Positive), so this step is to drop Netral label
```
"""

df.head()

# Filter rows where label is 1 or 3
num_label_1 = df[df['label'] == 1].shape[0]
label_1_data = df[df['label'].isin([1])]
label_3_data = df[df['label'] == 3].sample(n=num_label_1, random_state=0)

new_df  =pd.concat([label_3_data, label_1_data], axis=0)
# Switch the order of column and shuffle
new_df = new_df[['tweet_text', 'label']].sample(frac=1)
new_df

# Reset the index
new_df.reset_index(drop=True, inplace=True)
new_df

"""### Check Duplicate Data"""

# Check for duplicate values in 'tweet_text' column
duplicate_tweets = new_df[new_df.duplicated(subset=['tweet_text'], keep=False)]
duplicate_tweets

"""### Encoding Label

```
# Change value of label negatif 1 to 0 and label positif 3 to 1
```


"""

# Replace values in 'label' column
new_df['label'] = new_df['label'].replace({1: 0, 3: 1})
new_df

"""### Visualization the data after encode"""

new_df.head()

import seaborn as sns
import matplotlib.pyplot as plt

#GRAPHIC

# Plotting
plt.figure(figsize=(6, 5))

# Get the counts of each label
counts_series = new_df['label'].value_counts()
counts_dict = counts_series.to_dict()

# Plot the count plot
plot = sns.countplot(x='label', hue='label', data=new_df, palette='viridis', dodge=False)
# Add text annotations
for label, count in counts_dict.items():
    plt.text(label, count, str(count), ha='center', va='bottom', fontsize=8)

# Add title and labels to the plot
plt.title('Count of Data Sentiment')
plt.xlabel('Labels')
plt.ylabel('Count')

# Set custom tick labels for x-axis
plt.xticks(ticks=[0, 1],labels=['0', '1'])

# Add legend
handles, labels = plot.get_legend_handles_labels()

labels = ['Negative','Positive']
plt.legend(handles, labels, title='Labels')
# Display the plot
plt.show()

"""# Text Preprocessing

### Text Cleansing
"""

#Test Cleaning
import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
nltk.download('punkt')
nltk.download('wordnet')
from collections import Counter

# Function to remove Url
def remove_urls(text):
    # Define the pattern to match URLs
    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    # Remove URLs from the text using the pattern
    return url_pattern.sub(r'', text)

# Function to remove HTML tags
def remove_html_tags(text):
    clean = re.compile('<.*?>')
    return re.sub(clean, '', text)

# Function to remove mentions (@)
def remove_mentions(text):
    return re.sub(r'@[A-Za-z0-9_]+', '', text)

# Function to remove hashtags (#), optional
def remove_hashtags(text):
    return re.sub(r'#([^\s]+)', '', text)

# Function to remove special characters and numbers, keep only alphabetical characters
def remove_special_characters(text):
    return re.sub(r'[^a-zA-Z\s]', '', text)

# Function to convert text to lowercase
def convert_to_lowercase(text):
    return text.lower()

"""### Apply Function
to compare cleaned and un-cleaned data.
"""

# Apply all cleaning functions to 'tweet_text' column
# cleaned tweet yang sudah bersih dari simbol"
new_df['cleaned_tweet'] = new_df['tweet_text'].apply(remove_urls)
new_df['cleaned_tweet'] = new_df['cleaned_tweet'].apply(remove_html_tags)
new_df['cleaned_tweet'] = new_df['cleaned_tweet'].apply(remove_mentions)
new_df['cleaned_tweet'] = new_df['cleaned_tweet'].apply(remove_hashtags)
new_df['cleaned_tweet'] = new_df['cleaned_tweet'].apply(remove_special_characters)
new_df['cleaned_tweet'] = new_df['cleaned_tweet'].apply(convert_to_lowercase)
new_df

"""### Tokenization



"""

# Tokenize the 'cleaned_tweet' column using NLTK's word_tokenize
new_df['tokens'] = new_df['cleaned_tweet'].apply(word_tokenize)
new_df

"""### Remove Stop words"""

# Function to remove stop words using NLTK's stopwords list
stop_words = set(stopwords.words('english'))  # Set of English stop words
def remove_stop_words(tokens):
    return [token for token in tokens if token.lower() not in stop_words]
new_df['stopwords_tokens'] = new_df['tokens'].apply(remove_stop_words)

# new_df = new_df.drop('tokens_with_stopwords', axis=1)

new_df.head()

"""### Stemming and Lemmatization"""

# Initialize stemming and lemmatization objects
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

# Function for stemming
def apply_stemming(tokens):
    return [stemmer.stem(token) for token in tokens]

# Function for lemmatization
def apply_lemmatization(tokens):
    return [lemmatizer.lemmatize(token) for token in tokens]

# Apply stemming, and lemmatization
new_df['stemmed_tokens'] = new_df['stopwords_tokens'].apply(apply_stemming)
new_df['lemmatized_tokens'] = new_df['stopwords_tokens'].apply(apply_lemmatization)

new_df

"""#### export datset cleaned_data"""

new_df.to_csv("cleaned_data.csv")

"""# Wordcloud

### Count Word Frequently
"""

# Function to count word frequency across the entire DataFrame
def count_word_frequency(new_df):
    # Initialize a Counter object
    word_freq = Counter()

    # Iterate over each list of tokens in the DataFrame and update the Counter
    for tokens_list in new_df['lemmatized_tokens']:
        word_freq.update(tokens_list)

    # Convert Counter to a DataFrame
    word_freq_df = pd.DataFrame(list(word_freq.items()), columns=['Word', 'Frequency'])

    # Sort DataFrame by frequency in descending order
    word_freq_df = word_freq_df.sort_values(by='Frequency', ascending=False).reset_index(drop=True)

    return word_freq_df

# Apply word frequency counting across the entire DataFrame
word_freq_df = count_word_frequency(new_df)
word_freq_df.head()

from wordcloud import WordCloud

allWords = ''.join([twts for twts in new_df['cleaned_tweet']])
wordCloud = WordCloud(width = 500,height =300,random_state = 21,max_font_size =
119).generate(allWords)
plt.imshow(wordCloud,interpolation = "bilinear")
plt.axis('off')
plt.show()

"""# Vectorization and Model

### shortcut to Load Clean Dataset
"""

c_df = pd.read_csv("cleaned_data.csv", encoding= 'unicode_escape')
c_df

"""### export cleaned_data_tweet_only.csv"""

c_df = pd.read_csv("cleaned_data.csv", encoding= 'unicode_escape')
s_df = c_df[["label","cleaned_tweet"]]
s_df

s_df.to_csv("cleaned_data_tweet_only.csv")

"""## Option 1 (use lemmatization)

### TF-IDF
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score
from sklearn.svm import LinearSVC

"""I use lemmatized_tokens for development in machine learning."""

# Initialize TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer()

# Fit and transform the lemmatized tokens into TF-IDF vectors
X = tfidf_vectorizer.fit_transform(new_df['lemmatized_tokens'].apply(lambda tokens: ' '.join(tokens)))

# Convert the label to numerical values (assuming 'label' column contains the target classes)
y = new_df['label']

# Print the shape of X and y to verify dimensions
print("Shape of X:", X.shape)
print("Shape of y:", y.shape)

"""### export tfidf_vectorizer.sav"""

import pickle
from sklearn import model_selection
# save the model to disk
filename = 'tfidf_vectorizer.sav'
pickle.dump(tfidf_vectorizer, open(filename, 'wb'))

"""### Logistic Regression"""

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

# Initialize Logistic Regression model
logreg = LogisticRegression(max_iter=1000)

# Train the model on the training data
logreg.fit(X_train, y_train)

# Predict on the test data
y_pred = logreg.predict(X_test)

# Evaluate the model
print("Classification Report:")
print(classification_report(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))

# save the model to disk
filename = 'logregr.sav'
pickle.dump(logreg, open(filename, 'wb'))

"""dari hasil klasifiksi report dihasilkan akurasi 77% berdasarkan  dari total data 840 dari msing" lebel negatif 420 dan positif 420 degan splitting sekitar 20% menunjukan bahwa model algoritma logistic regression  cukup baik meskipun dari data latih yang tidak terlalu banyak.

## Option 2 (use stemming)
"""

# Initialize TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer()

# Fit and transform the lemmatized tokens into TF-IDF vectors
X = tfidf_vectorizer.fit_transform(new_df['stemmed_tokens'].apply(lambda tokens: ' '.join(tokens)))

# Convert the label to numerical values (assuming 'label' column contains the target classes)
y = new_df['label']

# Print the shape of X and y to verify dimensions
print("Shape of X:", X.shape)
print("Shape of y:", y.shape)

"""### Logistic Regression"""

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

# Initialize Logistic Regression model
logreg = LogisticRegression(max_iter=1000)

# Train the model on the training data
logreg.fit(X_train, y_train)

# Predict on the test data
y_pred = logreg.predict(X_test)

# Evaluate the model
print("Classification Report:")
print(classification_report(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))